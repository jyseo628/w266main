{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character based LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: run this command to allow jupyter to use the custom conda environment\n",
    "\n",
    "```shell\n",
    "ipython kernel install --user --name=char_lstm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see the provided requirements.txt, this includes all libraries needed to run this code in python3.6 using conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data utils contains all the file handling and conversion of sentiment train/test files from Latin1 to utf8. \n",
    "It also contains code to shuffle and split the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from ops import *\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "\n",
    "TRAIN_SET = TextReader.TRAIN_SET\n",
    "TEST_SET = TextReader.TEST_SET\n",
    "DEV_SET = TextReader.DEV_SET\n",
    "QA_SET = TextReader.QA_SET # Subset of dev_set limited to 100 rows for quick tests\n",
    "\n",
    "SAVE_PATH = PATH + 'checkpoints/lstm'\n",
    "LOGGING_PATH = PATH + 'checkpoints/log.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST mini-batching using DEV set using the TextReader from the data_utils library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "    0. 0. 0. 0.]]]]\n",
      "[[1 0]]\n",
      "[[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "  [[0. 0. 0. ... 0. 0. 0.]]]]\n",
      "[[1 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_word_length = 1\n",
    "\n",
    "with open(TextReader.DEV_SET, 'r',encoding='utf8') as f:\n",
    "    reader = TextReader(f, max_word_length)\n",
    "    n_samples = 2\n",
    "    batch_size = 1\n",
    "    n_batch = int(n_samples // batch_size)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "        if reader.load_to_ram(batch_size):                \n",
    "            a,b = reader.make_minibatch(reader.data)\n",
    "            print(a)\n",
    "            print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check various functions for data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode sentence:  hi there\n",
      "one hot:  (array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]]), 2)\n"
     ]
    }
   ],
   "source": [
    "t = TextReader(None,50)\n",
    "sentence = 'Hi there'\n",
    "print('encode sentence: ',t.encode_sentence(sentence))\n",
    "print('one hot: ',t.encode_one_hot(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch:  (array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.],\n",
      "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32), array([[1, 0],\n",
      "       [1, 0],\n",
      "       [0, 1]]))\n"
     ]
    }
   ],
   "source": [
    "sentences = ['Hi there ,1 ','How are you doing today ,1 ','\"How is the weather in Boston?\", 0']\n",
    "print('mini_batch: ', t.make_minibatch(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check file to ensure formatted correctly and count lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eof line count @ 10\n"
     ]
    }
   ],
   "source": [
    "with open(QA_SET, 'r', encoding = \"utf8\") as f:\n",
    "    t=TextReader(f,5)\n",
    "    t.check_file(TextReader.QA_SET, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check and iterate over a minibatch ( running one-hot encoding ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  sure thing should start with ustream amp blip channel or go big right out of gate buy brick amp mortar twittv,4 \n",
      "Sentiment:  4 \n",
      "Sentence:  sure thing should start with ustream amp blip channel or go big right out of gate buy brick amp mortar twittv\n",
      "one_hot shape: (21, 16, 70)\n",
      "batch:  0\n",
      "-----------------------------------------------------\n",
      "Original:  thank for video for alo girl tonight enjoy veri much,4 \n",
      "Sentiment:  4 \n",
      "Sentence:  thank for video for alo girl tonight enjoy veri much\n",
      "one_hot shape: (10, 16, 70)\n",
      "batch:  1\n",
      "-----------------------------------------------------\n",
      "Original:  so sad didnt camera while burn,0 \n",
      "Sentiment:  0 \n",
      "Sentence:  so sad didnt camera while burn\n",
      "one_hot shape: (6, 16, 70)\n",
      "batch:  2\n",
      "-----------------------------------------------------\n",
      "Original:  not nice night to out on tile drive safe,0 \n",
      "Sentiment:  0 \n",
      "Sentence:  not nice night to out on tile drive safe\n",
      "one_hot shape: (9, 16, 70)\n",
      "batch:  3\n",
      "-----------------------------------------------------\n",
      "Original:  look for anyon gd to help with logo design pronto huge meal as reward,4 \n",
      "Sentiment:  4 \n",
      "Sentence:  look for anyon gd to help with logo design pronto huge meal as reward\n",
      "one_hot shape: (14, 16, 70)\n",
      "batch:  4\n",
      "-----------------------------------------------------\n",
      "Original:  weve got holiday rain,0 \n",
      "Sentiment:  0 \n",
      "Sentence:  weve got holiday rain\n",
      "one_hot shape: (4, 16, 70)\n",
      "batch:  5\n",
      "-----------------------------------------------------\n",
      "Original:  workout wick hard alway limp when finish,4 \n",
      "Sentiment:  4 \n",
      "Sentence:  workout wick hard alway limp when finish\n",
      "one_hot shape: (7, 16, 70)\n",
      "batch:  6\n",
      "-----------------------------------------------------\n",
      "Original:  damn u den just threw away,0 \n",
      "Sentiment:  0 \n",
      "Sentence:  damn u den just threw away\n",
      "one_hot shape: (6, 16, 70)\n",
      "batch:  7\n",
      "-----------------------------------------------------\n",
      "Original:  ye isnt interest own mom made day about can terrif,4 \n",
      "Sentiment:  4 \n",
      "Sentence:  ye isnt interest own mom made day about can terrif\n",
      "one_hot shape: (10, 16, 70)\n",
      "batch:  8\n",
      "-----------------------------------------------------\n",
      "Original:  cant wait to get your best ohh def drove n half hour to find out tb concert can sad day,0 \n",
      "Sentiment:  0 \n",
      "Sentence:  cant wait to get your best ohh def drove n half hour to find out tb concert can sad day\n",
      "one_hot shape: (20, 16, 70)\n",
      "batch:  9\n",
      "-----------------------------------------------------\n",
      "Original:  willi walsh forgo juli salari of Ã£Â‚Ã¢Â£61000 sure pay year would help rich peopl eh\t4 \n",
      "Sentiment:  4 \n",
      "Sentence:  willi walsh forgo juli salari of Ã£Â‚Ã¢Â£61000 sure pay year would help rich peopl eh\n",
      "one_hot shape: (15, 16, 70)\n",
      "batch:  10\n",
      "-----------------------------------------------------\n",
      "Original:  Ã£Â¦Ã¢Â²Ã¢Â¹Ã£Â¨Ã¢Â²Ã¢Â“Ã£Â©Ã¢Â¤Ã¢ = d0nt kn0w mean but l00k pretti s0there rand0m\t4 \n",
      "Sentiment:  4 \n",
      "Sentence:  Ã£Â¦Ã¢Â²Ã¢Â¹Ã£Â¨Ã¢Â²Ã¢Â“Ã£Â©Ã¢Â¤Ã¢ = d0nt kn0w mean but l00k pretti s0there rand0m\n",
      "one_hot shape: (9, 16, 70)\n",
      "batch:  11\n",
      "-----------------------------------------------------\n",
      "Original:  oh english quit good donÃ£Â‚Ã¢Â´t know if sim run on comput becaus itÃ£Â‚Ã¢Â´ alreadi year old\t4 \n",
      "Sentiment:  4 \n",
      "Sentence:  oh english quit good donÃ£Â‚Ã¢Â´t know if sim run on comput becaus itÃ£Â‚Ã¢Â´ alreadi year old\n",
      "one_hot shape: (17, 16, 70)\n",
      "batch:  12\n",
      "-----------------------------------------------------\n",
      "Original:  nÃ£ÂƒÃ¢Â³i hix mÃ£ÂƒÃ¢Âºn nghe nhÃ£Ã¢ÂºÃ¢c miÃ£Ã¢n sÃ£ÂƒÃ¢Â´ng nÃ£Â†Ã¢Â°Ã£Ã¢Ã¢Â›c quÃ£ÂƒÃ¢ ai cÃ£ÂƒÃ¢Â³ list nÃ£ÂƒÃ¢ o hay share em Ã£ÂƒÃ¢Â­t vÃ£Ã¢Ã¢Â›i\t0 \n",
      "Sentiment:  0 \n",
      "Sentence:  nÃ£ÂƒÃ¢Â³i hix mÃ£ÂƒÃ¢Âºn nghe nhÃ£Ã¢ÂºÃ¢c miÃ£Ã¢n sÃ£ÂƒÃ¢Â´ng nÃ£Â†Ã¢Â°Ã£Ã¢Ã¢Â›c quÃ£ÂƒÃ¢ ai cÃ£ÂƒÃ¢Â³ list nÃ£ÂƒÃ¢ o hay share em Ã£ÂƒÃ¢Â­t vÃ£Ã¢Ã¢Â›i\n",
      "one_hot shape: (26, 16, 70)\n",
      "batch:  13\n",
      "-----------------------------------------------------\n",
      "Original:  great song bit revolutionari Ã£Â¢Ã¢Â™Ã¢ httpblipfm~7dyfi\t4 \n",
      "Sentiment:  4 \n",
      "Sentence:  great song bit revolutionari Ã£Â¢Ã¢Â™Ã¢ httpblipfm~7dyfi\n",
      "one_hot shape: (5, 16, 70)\n",
      "batch:  14\n",
      "-----------------------------------------------------\n",
      "Original:  water fight in park so cool c ever promes guy tu montrera cque ta achetÃ£ÂƒÃ¢Â©sc soir te ou love\t0 \n",
      "Sentiment:  0 \n",
      "Sentence:  water fight in park so cool c ever promes guy tu montrera cque ta achetÃ£ÂƒÃ¢Â©sc soir te ou love\n",
      "one_hot shape: (20, 16, 70)\n",
      "batch:  15\n",
      "-----------------------------------------------------\n",
      "Original:  asi se to nedÃ£ÂƒÃ¢ jinak nazvat protoÃ£ Ã¢Â¾e nenÃ£ÂƒÃ¢Â­ schopnÃ£ÂƒÃ¢Â© vysvÃ£Â„Ã¢Â›tlenÃ£ÂƒÃ¢Â­ pak Ã£ Ã¢Â¾e se nedÃ£Â„Ã¢Â›jÃ£ÂƒÃ¢Â­\t4 \n",
      "Sentiment:  4 \n",
      "Sentence:  asi se to nedÃ£ÂƒÃ¢ jinak nazvat protoÃ£ Ã¢Â¾e nenÃ£ÂƒÃ¢Â­ schopnÃ£ÂƒÃ¢Â© vysvÃ£Â„Ã¢Â›tlenÃ£ÂƒÃ¢Â­ pak Ã£ Ã¢Â¾e se nedÃ£Â„Ã¢Â›jÃ£ÂƒÃ¢Â­\n",
      "one_hot shape: (17, 16, 70)\n",
      "batch:  16\n",
      "-----------------------------------------------------\n",
      "Original:  Ã£ÂÃ¢Â¼Ã£ÂÃ¢ÂµÃ£ÂÃ¢Â½Ã£Â‘ Ã£ÂÃ¢Â±Ã£ÂÃ¢ÂµÃ£Â‘Ã£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚ Ã£ÂÃ¢Â´Ã£ÂÃ¢Â¸Ã£ÂÃ¢Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¹Ã£ÂÃ¢Â½ Ã£Â‘Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¹Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â¾Ã£ÂÃ¢Â² Ã£ÂÃ¢Ã£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â°Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢ÂŒ Ã£Â‘Ã£ÂÃ¢Â¾Ã£ÂÃ¢Â²Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â€Ã£Â‘Ã¢ÂˆÃ£ÂÃ¢ÂµÃ£ÂÃ¢Â½Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£Â‘ Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¸Ã£Â‘Ã¢ Ã£ÂÃ¢Â½Ã£ÂÃ¢Âµ Ã£Â‘Ã¢ÂƒÃ£ÂÃ¢Â´Ã£ÂÃ¢Â¾Ã£ÂÃ¢Â±Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£ÂÃ¢Â’Ã£ÂÃ¢Â¾Ã£ÂÃ¢ÂºÃ£Â‘Ã¢Â€Ã£Â‘Ã¢ÂƒÃ£ÂÃ¢Â³ Ã£Â‘Ã¢Â‚Ã£ÂÃ¢ÂµÃ£ÂÃ¢ÂºÃ£Â‘Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â° Ã£ÂÃ¢Â²Ã£Â‘Ã£ÂÃ¢Âµ Ã£ÂÃ¢Â¾Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â²Ã£ÂÃ¢Ã£ÂÃ¢ÂµÃ£ÂÃ¢ÂºÃ£ÂÃ¢Â°Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â‚ Ã£ÂÃ¢Â¸ Ã£ÂÃ¢Â¼Ã£ÂÃ¢ÂµÃ£ÂÃ¢Ã£Â‘Ã¢ÂŒÃ£Â‘Ã¢Â‚Ã£ÂÃ¢Â¸Ã£Â‘Ã¢ÂˆÃ£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚ Ã£ÂÃ¢Â”Ã£Â‘Ã¢Â€Ã£Â‘Ã¢ÂƒÃ£ÂÃ¢Â³Ã£ÂÃ¢Â¾Ã£ÂÃ¢Âµ Ã£ÂÃ¢Â´Ã£ÂÃ¢ÂµÃ£ÂÃ¢Ã£ÂÃ¢Â¾ rss+greader\t0 \n",
      "Sentiment:  0 \n",
      "Sentence:  Ã£ÂÃ¢Â¼Ã£ÂÃ¢ÂµÃ£ÂÃ¢Â½Ã£Â‘ Ã£ÂÃ¢Â±Ã£ÂÃ¢ÂµÃ£Â‘Ã£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚ Ã£ÂÃ¢Â´Ã£ÂÃ¢Â¸Ã£ÂÃ¢Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¹Ã£ÂÃ¢Â½ Ã£Â‘Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¹Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â¾Ã£ÂÃ¢Â² Ã£ÂÃ¢Ã£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â°Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢ÂŒ Ã£Â‘Ã£ÂÃ¢Â¾Ã£ÂÃ¢Â²Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â€Ã£Â‘Ã¢ÂˆÃ£ÂÃ¢ÂµÃ£ÂÃ¢Â½Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£Â‘ Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¸Ã£Â‘Ã¢ Ã£ÂÃ¢Â½Ã£ÂÃ¢Âµ Ã£Â‘Ã¢ÂƒÃ£ÂÃ¢Â´Ã£ÂÃ¢Â¾Ã£ÂÃ¢Â±Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£ÂÃ¢Â’Ã£ÂÃ¢Â¾Ã£ÂÃ¢ÂºÃ£Â‘Ã¢Â€Ã£Â‘Ã¢ÂƒÃ£ÂÃ¢Â³ Ã£Â‘Ã¢Â‚Ã£ÂÃ¢ÂµÃ£ÂÃ¢ÂºÃ£Â‘Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â° Ã£ÂÃ¢Â²Ã£Â‘Ã£ÂÃ¢Âµ Ã£ÂÃ¢Â¾Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â²Ã£ÂÃ¢Ã£ÂÃ¢ÂµÃ£ÂÃ¢ÂºÃ£ÂÃ¢Â°Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â‚ Ã£ÂÃ¢Â¸ Ã£ÂÃ¢Â¼Ã£ÂÃ¢ÂµÃ£ÂÃ¢Ã£Â‘Ã¢ÂŒÃ£Â‘Ã¢Â‚Ã£ÂÃ¢Â¸Ã£Â‘Ã¢ÂˆÃ£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚ Ã£ÂÃ¢Â”Ã£Â‘Ã¢Â€Ã£Â‘Ã¢ÂƒÃ£ÂÃ¢Â³Ã£ÂÃ¢Â¾Ã£ÂÃ¢Âµ Ã£ÂÃ¢Â´Ã£ÂÃ¢ÂµÃ£ÂÃ¢Ã£ÂÃ¢Â¾ rss+greader\n",
      "one_hot shape: (1, 16, 70)\n",
      "batch:  17\n",
      "-----------------------------------------------------\n",
      "Original:  think Ã£ÂÃ¢Â²Ã£ÂÃ¢ÂµÃ£ÂÃ¢Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â‚ Ã£ÂÃ¢ÂºÃ£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¹Ã£Â‘Ã¢Â†Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¼ Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¸ Ã£ÂÃ¢Ã£ÂÃ¢Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â€Ã£ÂÃ¢ÂºÃ£ÂÃ¢Â° Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¸ Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â²Ã£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢Â‚Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â€Ã£ÂÃ¢Â° Ã£ÂÃ¢Â¼Ã£ÂÃ¢Â¾Ã£ÂÃ¢Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£Â‘Ã£ÂÃ¢Ã£ÂÃ¢Â¾Ã£ÂÃ¢ÂºÃ£ÂÃ¢Â¾Ã£ÂÃ¢Â¹Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£Â‘Ã¢Â€Ã£ÂÃ¢Â°Ã£ÂÃ¢Â±Ã£ÂÃ¢Â¾Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â°Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢ÂŒ Ã£ÂÃ¢Â¸ Ã£ÂÃ¢Â½Ã£ÂÃ¢Âµ Ã£ÂÃ¢Â¾Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â²Ã£ÂÃ¢Ã£ÂÃ¢ÂµÃ£ÂÃ¢ÂºÃ£ÂÃ¢Â°Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢ÂŒÃ£Â‘Ã£Â‘ httpplurkcompy3kqi\t4 \n",
      "Sentiment:  4 \n",
      "Sentence:  think Ã£ÂÃ¢Â²Ã£ÂÃ¢ÂµÃ£ÂÃ¢Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â‚ Ã£ÂÃ¢ÂºÃ£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¹Ã£Â‘Ã¢Â†Ã£ÂÃ¢Â°Ã£ÂÃ¢Â¼ Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¸ Ã£ÂÃ¢Ã£ÂÃ¢Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â€Ã£ÂÃ¢ÂºÃ£ÂÃ¢Â° Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¸ Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â²Ã£ÂÃ¢Â¸Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢Â‚Ã£ÂÃ¢ÂµÃ£Â‘Ã¢Â€Ã£ÂÃ¢Â° Ã£ÂÃ¢Â¼Ã£ÂÃ¢Â¾Ã£ÂÃ¢Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£Â‘Ã£ÂÃ¢Ã£ÂÃ¢Â¾Ã£ÂÃ¢ÂºÃ£ÂÃ¢Â¾Ã£ÂÃ¢Â¹Ã£ÂÃ¢Â½Ã£ÂÃ¢Â¾ Ã£Â‘Ã¢Â€Ã£ÂÃ¢Â°Ã£ÂÃ¢Â±Ã£ÂÃ¢Â¾Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â°Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢ÂŒ Ã£ÂÃ¢Â¸ Ã£ÂÃ¢Â½Ã£ÂÃ¢Âµ Ã£ÂÃ¢Â¾Ã£Â‘Ã¢Â‚Ã£ÂÃ¢Â²Ã£ÂÃ¢Ã£ÂÃ¢ÂµÃ£ÂÃ¢ÂºÃ£ÂÃ¢Â°Ã£Â‘Ã¢Â‚Ã£Â‘Ã¢ÂŒÃ£Â‘Ã£Â‘ httpplurkcompy3kqi\n",
      "one_hot shape: (2, 16, 70)\n",
      "batch:  18\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nephila\\Anaconda3\\envs\\char_lstm\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: generator 'TextReader.iterate_minibatch' raised StopIteration\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "with open(QA_SET, 'r', encoding = \"utf8\") as f:\n",
    "    t=TextReader(f,16,True)\n",
    "    for n, x in enumerate(t.iterate_minibatch(1, TextReader.QA_SET)):\n",
    "        print('batch: ',n)\n",
    "        print('-----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup experiment tracking server ( using Databricks mlflow ), accessible via http://35.231.123.5:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "import mlflow.tensorflow\n",
    "\n",
    "mlflow_server = '35.231.123.5'\n",
    "mlflow_tracking_URI = 'http://' + mlflow_server + ':5000'\n",
    "mlflow.set_tracking_uri(mlflow_tracking_URI)\n",
    "\n",
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up experiment to track results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment_id:  2\n"
     ]
    }
   ],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment = client.get_experiment_by_name(name='train_char_lstm') \n",
    "print('experiment_id: ',experiment.experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset stdout (as file handling sometimes results in output not being returned from console to jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare class that manages the LSTM - build, train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "class LSTM(object):\n",
    "    \n",
    "    \"\"\" Character-Level LSTM Implementation \"\"\"\n",
    "\n",
    "    ENCODING = 'utf8'\n",
    "    \n",
    "    def __init__(self, experiment_id):\n",
    "        \n",
    "        # X is of shape ('b', 'sentence_length', 'max_word_length', 'alphabet_size')\n",
    "        self.hparams = self.get_hparams()\n",
    "        max_word_length = self.hparams['max_word_length']\n",
    "        self.X = tf.placeholder('float32', shape=[None, None, max_word_length, ALPHABET_SIZE], name='X')\n",
    "        self.Y = tf.placeholder('float32', shape=[None, 2], name='Y')\n",
    "\n",
    "        self.experiment_id = experiment_id\n",
    "            \n",
    "    def build(self,\n",
    "              training,\n",
    "              testing_batch_size,\n",
    "              kernels,\n",
    "              kernel_features,\n",
    "              rnn_size,\n",
    "              dropout,\n",
    "              size,\n",
    "              train_samples,\n",
    "              valid_samples):\n",
    "\n",
    "        self.size = size\n",
    "        self.hparams = self.get_hparams()\n",
    "        self.max_word_length = self.hparams['max_word_length']\n",
    "        self.train_samples = train_samples\n",
    "        self.valid_samples = valid_samples\n",
    "        \n",
    "        if training == True:\n",
    "            BATCH_SIZE = self.hparams['BATCH_SIZE']\n",
    "            self.BATCH_SIZE = BATCH_SIZE\n",
    "        else:\n",
    "            BATCH_SIZE = testing_batch_size\n",
    "            self.BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "        # Highway & TDNN Implementation are from https://github.com/mkroutikov/tf-lstm-char-cnn/blob/master/model.py\n",
    "        def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "            \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "            t = sigmoid(Wy + b)\n",
    "            z = t * g(Wy + b) + (1 - t) * y\n",
    "            where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "            \"\"\"\n",
    "\n",
    "            with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "                for idx in range(num_layers):\n",
    "                    g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "                    t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "                    output = t * g + (1. - t) * input_\n",
    "                    input_ = output\n",
    "\n",
    "            return output\n",
    "\n",
    "        def tdnn(input_, kernels, kernel_features, scope='TDNN'):\n",
    "            ''' Time Delay Neural Network\n",
    "            :input:           input float tensor of shape [(batch_size*num_unroll_steps) x max_word_length x embed_size]\n",
    "            :kernels:         array of kernel sizes\n",
    "            :kernel_features: array of kernel feature sizes (parallel to kernels)\n",
    "            '''\n",
    "            assert len(kernels) == len(kernel_features), 'Kernel and Features must have the same size'\n",
    "\n",
    "            # input_ is a np.array of shape ('b', 'sentence_length', 'max_word_length', 'embed_size') we\n",
    "            # need to convert it to shape ('b * sentence_length', 1, 'max_word_length', 'embed_size') to\n",
    "            # use conv2D\n",
    "            input_ = tf.reshape(input_, [-1, self.max_word_length, ALPHABET_SIZE])\n",
    "            input_ = tf.expand_dims(input_, 1)\n",
    "\n",
    "            layers = []\n",
    "            with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "                for kernel_size, kernel_feature_size in zip(kernels, kernel_features):\n",
    "                    reduced_length = self.max_word_length - kernel_size + 1\n",
    "\n",
    "                    # [batch_size * sentence_length x max_word_length x embed_size x kernel_feature_size]\n",
    "                    conv = conv2d(input_, kernel_feature_size, 1, kernel_size, \"kernel_%d\" % kernel_size)\n",
    "\n",
    "                    # [batch_size * sentence_length x 1 x 1 x kernel_feature_size]\n",
    "                    pool = tf.nn.max_pool(tf.tanh(conv), [1, 1, reduced_length, 1], [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "                    layers.append(tf.squeeze(pool, [1, 2]))\n",
    "\n",
    "                if len(kernels) > 1:\n",
    "                    output = tf.concat(layers, 1)\n",
    "                else:\n",
    "                    output = layers[0]\n",
    "\n",
    "            return output\n",
    "\n",
    "        cnn = tdnn(self.X, kernels, kernel_features)\n",
    "\n",
    "        # tdnn() returns a tensor of shape [batch_size * sentence_length x kernel_features]\n",
    "        # highway() returns a tensor of shape [batch_size * sentence_length x size] to use\n",
    "        # tensorflow dynamic_rnn module we need to reshape it to [batch_size x sentence_length x size]\n",
    "        cnn = highway(cnn, self.size)\n",
    "        cnn = tf.reshape(cnn, [BATCH_SIZE, -1, self.size])\n",
    "\n",
    "        with tf.variable_scope('LSTM', reuse=tf.AUTO_REUSE):\n",
    "\n",
    "            # Upgrade to this library\n",
    "            # tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell')\n",
    "            \n",
    "            def create_rnn_cell():\n",
    "                cell = rnn.BasicLSTMCell(rnn_size, state_is_tuple=True, forget_bias=0.0, reuse=False)\n",
    "\n",
    "                if dropout > 0.0:\n",
    "                    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=1. - dropout)\n",
    "\n",
    "                return cell\n",
    "\n",
    "            cell = create_rnn_cell()\n",
    "            initial_rnn_state = cell.zero_state(BATCH_SIZE, dtype='float32')\n",
    "\n",
    "            outputs, final_rnn_state = tf.nn.dynamic_rnn(cell, cnn,\n",
    "                                                         initial_state=initial_rnn_state,\n",
    "                                                         dtype=tf.float32)\n",
    "\n",
    "            # In this implementation, we only care about the last outputs of the RNN\n",
    "            # i.e. the output at the end of the sentence\n",
    "            outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "            last = outputs[-1]\n",
    "\n",
    "        self.prediction = softmax(last, 2)\n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        BATCH_SIZE = self.hparams['BATCH_SIZE']\n",
    "        EPOCHS = self.hparams['EPOCHS']\n",
    "        max_word_length = self.hparams['max_word_length']\n",
    "        learning_rate = self.hparams['learning_rate']\n",
    "\n",
    "        pred = self.prediction\n",
    "\n",
    "        cost = - tf.reduce_sum(self.Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(self.Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        n_batch = self.train_samples // BATCH_SIZE\n",
    "\n",
    "        # parameters for saving and early stopping\n",
    "        saver = tf.train.Saver()\n",
    "        patience = self.hparams['patience']\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            best_acc = 0.0\n",
    "            DONE = False\n",
    "            epoch = 0\n",
    "         \n",
    "            # Ensure prior run has been finished\n",
    "            mlflow.end_run()\n",
    "        \n",
    "            # Start experiment\n",
    "            with mlflow.start_run(experiment_id = self.experiment_id):\n",
    "                \n",
    "                while epoch <= EPOCHS and not DONE:\n",
    "                    loss = 0.0\n",
    "                    batch = 1\n",
    "                    epoch += 1\n",
    "\n",
    "                    print(\"epoch %d\" % epoch)\n",
    "\n",
    "                    with open(TRAIN_SET, 'r', encoding = LSTM.ENCODING) as f:\n",
    "                                                    \n",
    "                        reader = TextReader(f, max_word_length)\n",
    "                        \n",
    "                        for minibatch in reader.iterate_minibatch(BATCH_SIZE, dataset=TextReader.TRAIN_SET):\n",
    "                            batch_x, batch_y = minibatch\n",
    "\n",
    "                            _, c, a = sess.run([optimizer, cost, acc], feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "\n",
    "                            loss += c\n",
    "\n",
    "                            if batch % 100 == 0:\n",
    "                                # Compute Accuracy on the Training set and print some info\n",
    "                                \n",
    "                                mlflow.log_metric(\"epoch\",epoch)\n",
    "                                mlflow.log_metric(\"batch\",batch) \n",
    "                                mlflow.log_metric(\"loss\",loss/batch)\n",
    "                                mlflow.log_metric(\"accuracy\",a)\n",
    "                                \n",
    "                                print(\"Epoch: %5d/%5d -- batch: %5d/%5d -- Loss: %.4f -- Train Accuracy: %.4f\" %\n",
    "                                      (epoch, EPOCHS, batch, n_batch, loss/batch, a))\n",
    "\n",
    "                            # --------------\n",
    "                            # EARLY STOPPING\n",
    "                            # --------------\n",
    "\n",
    "                            # Compute Accuracy on the Validation set, check if validation has improved, save model, etc\n",
    "                            if batch % 500 == 0:\n",
    "                                accuracy = []\n",
    "\n",
    "                                # Validation set is very large, so accuracy is computed on testing set\n",
    "                                # instead of valid set, change TEST_SET to VALID_SET to compute accuracy on valid set\n",
    "                                with open(TEST_SET, 'r', encoding = LSTM.ENCODING) as ff:\n",
    "                                                                        \n",
    "                                    valid_reader = TextReader(ff, max_word_length)\n",
    "                                    \n",
    "                                    for mb in valid_reader.iterate_minibatch(BATCH_SIZE, dataset=TextReader.TEST_SET):\n",
    "                                        valid_x, valid_y = mb\n",
    "                                        a = sess.run([acc], feed_dict={self.X: valid_x, self.Y: valid_y})\n",
    "                                        accuracy.append(a)\n",
    "                                    \n",
    "                                    mean_acc = np.mean(accuracy)\n",
    "\n",
    "                                    # if accuracy has improved, save model and boost patience\n",
    "                                    if mean_acc > best_acc:\n",
    "                                        best_acc = mean_acc\n",
    "                                        save_path = saver.save(sess, SAVE_PATH)\n",
    "                                        patience = self.hparams['patience']\n",
    "                                        print('Model saved in file: %s' % save_path)\n",
    "                                        mlflow.log_metric(\"epoch\",epoch)\n",
    "                                        mlflow.log_metric(\"best_acc\",mean_acc)\n",
    "                                        \n",
    "                                    # else reduce patience and break loop if necessary\n",
    "                                    else:                                        \n",
    "                                        mlflow.log_metric(\"patience\",patience)\n",
    "                                        patience -= 500\n",
    "                                        if patience <= 0:\n",
    "                                            DONE = True\n",
    "                                            break\n",
    "\n",
    "                                    mlflow.log_metric(\"epoch\",epoch)\n",
    "                                    mlflow.log_metric(\"batch\",batch) \n",
    "                                    mlflow.log_metric(\"mean_acc\",mean_acc)\n",
    "                                    \n",
    "                                    print('Epoch: %5d/%5d -- batch: %5d/%5d -- Valid Accuracy: %.4f' %\n",
    "                                         (epoch, EPOCHS, batch, n_batch, mean_acc))\n",
    "\n",
    "                            batch += 1\n",
    "                            \n",
    "\n",
    "    def evaluate_test_set(self):\n",
    "        '''\n",
    "        Evaluate Test Set\n",
    "        On a model that trained for around 5 epochs it achieved:\n",
    "        # Valid loss: 23.50035 -- Valid Accuracy: 0.83613\n",
    "        '''\n",
    "        BATCH_SIZE = self.hparams['BATCH_SIZE']\n",
    "        max_word_length = self.hparams['max_word_length']\n",
    "\n",
    "        pred = self.prediction\n",
    "\n",
    "        cost = - tf.reduce_sum(self.Y * tf.log(tf.clip_by_value(pred, 1e-10, 1.0)))\n",
    "\n",
    "        predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(self.Y, 1))\n",
    "\n",
    "        acc = tf.reduce_mean(tf.cast(predictions, 'float32'))\n",
    "\n",
    "        # parameters for restoring variables\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Ensure prior run has been finished\n",
    "        mlflow.end_run()\n",
    "        \n",
    "        # Start experiment\n",
    "        with mlflow.start_run(experiment_id = self.experiment_id):\n",
    "            with tf.Session() as sess:\n",
    "\n",
    "                print('Loading model %s...' % SAVE_PATH)\n",
    "                saver.restore(sess, SAVE_PATH)\n",
    "                print('Loaded')\n",
    "\n",
    "                loss = []\n",
    "                accuracy = []\n",
    "\n",
    "                with open(DEV_SET, 'r', encoding = LSTM.ENCODING) as f:\n",
    "                    \n",
    "                    reader = TextReader(f, max_word_length)\n",
    "                    \n",
    "                    for minibatch in reader.iterate_minibatch(BATCH_SIZE, dataset=TextReader.DEV_SET):\n",
    "                        batch_x, batch_y = minibatch\n",
    "                        c, a = sess.run([cost, acc], feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "                        loss.append(c)\n",
    "                        accuracy.append(a)\n",
    "\n",
    "                    loss = np.mean(loss)\n",
    "                    accuracy = np.mean(accuracy)\n",
    "                    \n",
    "                    mlflow.log_metric(\"loss\",loss)\n",
    "                    mlflow.log_metric(\"accuracy\",accuracy) \n",
    "                    \n",
    "                    print('Valid loss: %.5f -- Valid Accuracy: %.5f' % (loss, accuracy))\n",
    "                    \n",
    "                    return loss, accuracy\n",
    "\n",
    "    def predict_sentences(self, sentences):\n",
    "        '''\n",
    "        Analyze Some Sentences\n",
    "\n",
    "        :sentences: list of sentences\n",
    "        e.g.: sentences = ['this is veeeryyy bad!!', 'I don\\'t think he will be happy abt this',\n",
    "                            'YOU\\'re a fool!', 'I\\'m sooo happY!!!']\n",
    "\n",
    "        Sentence: \"this is veeeryyy bad!!\" , yielded results (pos/neg): 0.04511/0.95489, prediction: neg\n",
    "        Sentence: \"I dont think he will be happy abt this\" , yielded results (pos/neg): 0.05929/0.94071, prediction: neg\n",
    "        Sentence: \"YOUre such an incompetent fool!\" , yielded results (pos/neg): 0.48503/0.51497, prediction: neg ***\n",
    "        Sentence: \"Im sooo happY!!!\" , yielded results (pos/neg): 0.97455/0.02545, prediction: pos\n",
    "\n",
    "        '''\n",
    "        BATCH_SIZE = self.hparams['BATCH_SIZE']\n",
    "        max_word_length = self.hparams['max_word_length']\n",
    "        pred = self.prediction\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            print('Loading model %s...' % SAVE_PATH)\n",
    "            saver.restore(sess, SAVE_PATH)\n",
    "            print('Loaded')\n",
    "\n",
    "            # Add placebo value '0,' at the beginning of the sentences to\n",
    "            # use the make_minibatch() method\n",
    "            sentences = ['0,' + s for s in sentences]\n",
    "\n",
    "            with open(TEST_SET, 'r', encoding = LSTM.ENCODING) as f:\n",
    "                                        \n",
    "                reader = TextReader(file=f, max_word_length=max_word_length)\n",
    "                reader.load_to_ram(BATCH_SIZE)\n",
    "                reader.data[:len(sentences)] = sentences\n",
    "                batch_x, batch_y = reader.make_minibatch(reader.data)\n",
    "\n",
    "                p = sess.run([pred], feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "\n",
    "                for i, s in enumerate(sentences):\n",
    "                    print('Sentence: %s , yielded results (pos/neg): %.5f/%.5f, prediction: %s' %\n",
    "                          (s, p[0][i][0], p[0][i][1], 'pos' if max(p[0][i]) == p[0][i][0] else 'neg'))\n",
    "\n",
    "            return p\n",
    "\n",
    "    def categorize_sentences(self, sentences):\n",
    "        \"\"\" Op for categorizing multiple sentences (> BATCH_SIZE) \"\"\"\n",
    "        # encode sentences\n",
    "        sentences = [s.encode('utf-8') for s in sentences]\n",
    "\n",
    "        q = queue.Queue()\n",
    "        reader = TextReader(file=None, max_word_length=self.max_word_length)\n",
    "        n_batch = len(sentences) // self.BATCH_SIZE\n",
    "        pred = self.prediction\n",
    "        saver = tf.train.Saver()\n",
    "        results = []\n",
    "\n",
    "        def fill_list(list, length):\n",
    "            while len(list) != length:\n",
    "                list.append('empty sentence.')\n",
    "            return list\n",
    "\n",
    "        # Fill queue with minibatches\n",
    "        for i in range(n_batch + 1):\n",
    "            if i == n_batch:\n",
    "                q.put(fill_list(sentences, self.BATCH_SIZE))\n",
    "            else:\n",
    "                q.put(sentences[i * self.BATCH_SIZE: (i + 1) * self.BATCH_SIZE])\n",
    "\n",
    "        # Predict\n",
    "        with tf.Session() as sess:\n",
    "            print('Loading model %s...' % SAVE_PATH)\n",
    "            saver.restore(sess, SAVE_PATH)\n",
    "            print('Model loaded')\n",
    "\n",
    "            while not q.empty():\n",
    "                batch = q.get()\n",
    "                batch = ['0, ' + s for s in batch]\n",
    "                batch_x, batch_y = reader.make_minibatch(batch)\n",
    "                p = sess.run([pred], feed_dict={self.X: batch_x, self.Y: batch_y})\n",
    "                results.append(p)\n",
    "\n",
    "        return results\n",
    "                        \n",
    "    def get_hparams(self):\n",
    "        ''' Get Hyperparameters '''\n",
    "\n",
    "        return {\n",
    "            'BATCH_SIZE':       64,\n",
    "            'EPOCHS':           2,\n",
    "            'max_word_length':  32, #16\n",
    "            'learning_rate':    0.0001,\n",
    "            'patience':         10000,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset tensorflow graph and setup LSTM and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel_1\n",
      "kernel_2\n",
      "kernel_3\n",
      "kernel_4\n",
      "kernel_5\n",
      "kernel_6\n",
      "kernel_7\n"
     ]
    }
   ],
   "source": [
    "# Important note, the scaling of train/valid samples is not working correctly \n",
    "# as there is a hard dependency in data_utils, needs to be fixed so can train/test on smaller batches / # records\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "network = LSTM(experiment.experiment_id)\n",
    "\n",
    "network.build(training=True, \n",
    "              testing_batch_size=1000, \n",
    "              kernels= [1, 2, 3, 4, 5, 6, 7],\n",
    "              kernel_features= [25, 50, 75, 100, 125, 150, 175],                   \n",
    "              rnn_size=650,\n",
    "              dropout=0.0,\n",
    "              size=700,\n",
    "              train_samples=1024000,\n",
    "              valid_samples=320000)\n",
    "\n",
    "# Original config\n",
    "# network.build(\n",
    "#               training=True,\n",
    "#               testing_batch_size=1000,\n",
    "#               kernels= [1, 2, 3, 4, 5, 6, 7],\n",
    "#               kernel_features= [25, 50, 75, 100, 125, 150, 175],\n",
    "#               rnn_size=650,\n",
    "#               dropout=0.0,\n",
    "#               size=700,\n",
    "#               train_samples=1024000,\n",
    "#               valid_samples=320000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch:     1/    2 -- batch:   100/16000 -- Loss: 44.3966 -- Train Accuracy: 0.3750\n",
      "Epoch:     1/    2 -- batch:   200/16000 -- Loss: 44.3654 -- Train Accuracy: 0.5781\n",
      "Epoch:     1/    2 -- batch:   300/16000 -- Loss: 44.3496 -- Train Accuracy: 0.5625\n",
      "Epoch:     1/    2 -- batch:   400/16000 -- Loss: 44.3586 -- Train Accuracy: 0.4375\n",
      "Epoch:     1/    2 -- batch:   500/16000 -- Loss: 44.3629 -- Train Accuracy: 0.4688\n",
      "Model saved in file: ./checkpoints/lstm\n",
      "Epoch:     1/    2 -- batch:   500/16000 -- Valid Accuracy: 0.5055\n",
      "Epoch:     1/    2 -- batch:   600/16000 -- Loss: 44.3567 -- Train Accuracy: 0.5625\n",
      "Epoch:     1/    2 -- batch:   700/16000 -- Loss: 44.3592 -- Train Accuracy: 0.5469\n",
      "Epoch:     1/    2 -- batch:   800/16000 -- Loss: 44.3575 -- Train Accuracy: 0.5625\n",
      "Epoch:     1/    2 -- batch:   900/16000 -- Loss: 44.3561 -- Train Accuracy: 0.5781\n",
      "Epoch:     1/    2 -- batch:  1000/16000 -- Loss: 44.3462 -- Train Accuracy: 0.6250\n",
      "Model saved in file: ./checkpoints/lstm\n",
      "Epoch:     1/    2 -- batch:  1000/16000 -- Valid Accuracy: 0.5210\n",
      "Epoch:     1/    2 -- batch:  1100/16000 -- Loss: 44.3427 -- Train Accuracy: 0.5000\n",
      "Epoch:     1/    2 -- batch:  1200/16000 -- Loss: 44.3408 -- Train Accuracy: 0.5312\n",
      "Epoch:     1/    2 -- batch:  1300/16000 -- Loss: 44.3352 -- Train Accuracy: 0.5156\n",
      "Epoch:     1/    2 -- batch:  1400/16000 -- Loss: 44.3321 -- Train Accuracy: 0.5156\n",
      "Epoch:     1/    2 -- batch:  1500/16000 -- Loss: 44.3194 -- Train Accuracy: 0.5625\n",
      "Model saved in file: ./checkpoints/lstm\n",
      "Epoch:     1/    2 -- batch:  1500/16000 -- Valid Accuracy: 0.5463\n",
      "Epoch:     1/    2 -- batch:  1600/16000 -- Loss: 44.3084 -- Train Accuracy: 0.5156\n",
      "Epoch:     1/    2 -- batch:  1700/16000 -- Loss: 44.2449 -- Train Accuracy: 0.5469\n",
      "Epoch:     1/    2 -- batch:  1800/16000 -- Loss: 44.0428 -- Train Accuracy: 0.6250\n",
      "Epoch:     1/    2 -- batch:  1900/16000 -- Loss: 43.7839 -- Train Accuracy: 0.6406\n",
      "Epoch:     1/    2 -- batch:  2000/16000 -- Loss: 43.5026 -- Train Accuracy: 0.7188\n",
      "Model saved in file: ./checkpoints/lstm\n",
      "Epoch:     1/    2 -- batch:  2000/16000 -- Valid Accuracy: 0.6881\n",
      "Epoch:     1/    2 -- batch:  2100/16000 -- Loss: 43.2204 -- Train Accuracy: 0.7969\n",
      "Epoch:     1/    2 -- batch:  2200/16000 -- Loss: 42.9463 -- Train Accuracy: 0.7344\n",
      "Epoch:     1/    2 -- batch:  2300/16000 -- Loss: 42.6683 -- Train Accuracy: 0.6562\n",
      "Epoch:     1/    2 -- batch:  2400/16000 -- Loss: 42.3582 -- Train Accuracy: 0.7656\n",
      "Epoch:     1/    2 -- batch:  2500/16000 -- Loss: 42.0819 -- Train Accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "network.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate test set on trained LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.evaluate_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "char_lstm",
   "language": "python",
   "name": "char_lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
