{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Characters level ConvNet paper: https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\n",
    "# Reference: https://github.com/mhjabreel/CharCnn_Keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate, ThresholdedReLU, MaxPooling1D, Flatten, Dropout, ReLU, Activation\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/no_tags_lower_train_data.csv', encoding='ISO-8859-1', keep_default_na=False)\n",
    "test_data = pd.read_csv('../data/no_tags_lower_test_data.csv', encoding='ISO-8859-1', keep_default_na=False)\n",
    "dev_data = pd.read_csv('../data/no_tags_lower_dev_data.csv', encoding='ISO-8859-1', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 41,\n",
       " '\"': 45,\n",
       " '#': 51,\n",
       " '$': 52,\n",
       " '%': 53,\n",
       " '&': 55,\n",
       " \"'\": 44,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 56,\n",
       " '+': 59,\n",
       " ',': 38,\n",
       " '-': 60,\n",
       " '.': 40,\n",
       " '/': 46,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ':': 43,\n",
       " ';': 39,\n",
       " '<': 62,\n",
       " '=': 61,\n",
       " '>': 63,\n",
       " '?': 42,\n",
       " '@': 50,\n",
       " '[': 66,\n",
       " '\\\\': 47,\n",
       " ']': 67,\n",
       " '^': 54,\n",
       " '_': 49,\n",
       " '`': 58,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '{': 68,\n",
       " '|': 48,\n",
       " '}': 69,\n",
       " '~': 57}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "alphabet_size = len(alphabet)\n",
    "alphabet_index = {alphabet[i]: i + 1 for i in range(alphabet_size)}\n",
    "alphabet_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_input_size = max(len(row['text']) for _, row in train_data.iterrows())\n",
    "max_input_size = 374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_padding(df, alphabet_index, max_input_size):\n",
    "    X = []\n",
    "    for _, row in df.iterrows():\n",
    "        str2idx = np.zeros(max_input_size, dtype='int64')\n",
    "        for i, letter in enumerate(row['text'].lower()):\n",
    "            if i == max_input_size:\n",
    "                break\n",
    "            str2idx[i] = alphabet_index.get(letter, 0)\n",
    "        X.append(str2idx)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = text_to_padding(train_data, alphabet_index, max_input_size)\n",
    "X_test = text_to_padding(test_data, alphabet_index, max_input_size)\n",
    "X_dev = text_to_padding(dev_data, alphabet_index, max_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tranin: (1024000, 374)\n",
      "test: (320000, 374)\n",
      "dev: (256000, 374)\n"
     ]
    }
   ],
   "source": [
    "print('tranin: ' + str(X_train.shape))\n",
    "print('test: ' + str(X_test.shape))\n",
    "print('dev: ' + str(X_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(train_data['sentiment']).values\n",
    "Y_test = pd.get_dummies(test_data['sentiment']).values\n",
    "Y_dev = pd.get_dummies(dev_data['sentiment']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tranin: (1024000, 2)\n",
      "test: (320000, 2)\n",
      "dev: (256000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('tranin: ' + str(Y_train.shape))\n",
    "print('test: ' + str(Y_test.shape))\n",
    "print('dev: ' + str(Y_dev.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNNZhang(object):\n",
    "    \"\"\"\n",
    "    Class to implement the Character Level Convolutional Neural Network for Text Classification,\n",
    "    as described in Zhang et al., 2015 (http://arxiv.org/abs/1509.01626)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, alphabet_size, embedding_size,\n",
    "                 conv_layers, fully_connected_layers, num_of_classes,\n",
    "                 threshold, dropout_p, learning_rate, loss='categorical_crossentropy'):\n",
    "        \"\"\"\n",
    "        Initialization for the Character Level CNN model.\n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            alphabet_size (int): Size of alphabets to create embeddings for\n",
    "            embedding_size (int): Size of embeddings\n",
    "            conv_layers (list[list[int]]): List of Convolution layers for model\n",
    "            fully_connected_layers (list[list[int]]): List of Fully Connected layers for model\n",
    "            num_of_classes (int): Number of classes in data\n",
    "            threshold (float): Threshold for Thresholded ReLU activation function\n",
    "            dropout_p (float): Dropout Probability\n",
    "            optimizer (str): Training optimizer\n",
    "            loss (str): Loss function\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fully_connected_layers = fully_connected_layers\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.threshold = threshold\n",
    "        self.dropout_p = dropout_p\n",
    "        self.optimizer = Adam(learning_rate)\n",
    "        self.loss = loss\n",
    "        self._build_model()  # builds self.model variable\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile the Character Level CNN model\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        inputs = Input(shape=(self.input_size,), name='sent_input')\n",
    "        # Embedding layers\n",
    "        x = Embedding(self.alphabet_size + 1, self.embedding_size, input_length=self.input_size)(inputs)\n",
    "        # Convolution layers\n",
    "        for cl in self.conv_layers:\n",
    "            x = Convolution1D(cl[0], cl[1])(x)\n",
    "            x = ThresholdedReLU(self.threshold)(x)\n",
    "            if cl[2] != -1:\n",
    "                x = MaxPooling1D(cl[2])(x)\n",
    "        x = Flatten()(x)\n",
    "        # Fully connected layers\n",
    "        for fl in self.fully_connected_layers:\n",
    "            x = Dense(fl)(x)\n",
    "            x = ThresholdedReLU(self.threshold)(x)\n",
    "            x = Dropout(self.dropout_p)(x)\n",
    "        # Output layer\n",
    "        predictions = Dense(self.num_of_classes, activation='softmax')(x)\n",
    "        # Build and compile model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        # model = multi_gpu_model(model, 2, cpu_relocation=True)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        print(\"CharCNNZhang model built: \")\n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "    def train(self, training_inputs, training_labels, validation_data, epochs, batch_size):\n",
    "        \"\"\"\n",
    "        Training function\n",
    "        Args:\n",
    "            training_inputs (numpy.ndarray): Training set inputs\n",
    "            training_labels (numpy.ndarray): Training set labels\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size\n",
    "            checkpoint_every (int): Interval for logging to Tensorboard\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Start training\n",
    "        print(\"Training CharCNNZhang model: \")\n",
    "        return self.model.fit(training_inputs, training_labels,\n",
    "                       validation_data=validation_data,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size)\n",
    "\n",
    "    def test(self, testing_inputs, testing_labels, batch_size):\n",
    "        \"\"\"\n",
    "        Testing function\n",
    "        Args:\n",
    "            testing_inputs (numpy.ndarray): Testing set inputs\n",
    "            testing_labels (numpy.ndarray): Testing set labels\n",
    "            batch_size (int): Batch size\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Evaluate inputs\n",
    "        return self.model.evaluate(testing_inputs, testing_labels, batch_size=batch_size)\n",
    "        # self.model.predict(testing_inputs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharCNNZhang model built: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      (None, 374)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 374, 128)          8960      \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 368, 256)          229632    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_17 (Thresh (None, 368, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 122, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 116, 256)          459008    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_18 (Thresh (None, 116, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 38, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 36, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_19 (Thresh (None, 36, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 34, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_20 (Thresh (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 32, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_21 (Thresh (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 30, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_22 (Thresh (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              2622464   \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_23 (Thresh (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_24 (Thresh (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 5,159,170\n",
      "Trainable params: 5,159,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "conv_layers = [[256, 7, 3], [256, 7, 3], [256, 3, -1], [256, 3, -1], [256, 3, -1], [256, 3, 3]]\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 2\n",
    "threshold = 1e-6\n",
    "dropout_p = 0.5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = CharCNNZhang(max_input_size, alphabet_size, embedding_size,\n",
    "                 conv_layers, fully_connected_layers, num_of_classes,\n",
    "                 threshold, dropout_p, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharCNNZhang model: \n",
      "Train on 1024000 samples, validate on 256000 samples\n",
      "Epoch 1/4\n",
      " - 763s - loss: 0.5103 - acc: 0.7413 - val_loss: 0.4513 - val_acc: 0.7889\n",
      "Epoch 2/4\n",
      " - 720s - loss: 0.4238 - acc: 0.8057 - val_loss: 0.4304 - val_acc: 0.8035\n",
      "Epoch 3/4\n",
      " - 720s - loss: 0.3911 - acc: 0.8248 - val_loss: 0.3978 - val_acc: 0.8200\n",
      "Epoch 4/4\n",
      " - 718s - loss: 0.3658 - acc: 0.8386 - val_loss: 0.3972 - val_acc: 0.8228\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 256\n",
    "\n",
    "history = model.train(\n",
    "    training_inputs=X_train,\n",
    "    training_labels=Y_train,\n",
    "    validation_data=[X_dev, Y_dev],\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000/320000 [==============================] - 63s 197us/step\n",
      "score: 0.39518755\n",
      "acc: 0.82347812\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"score: %.8f\" % (score))\n",
    "print(\"acc: %.8f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharCNNZhang model: \n",
      "Train on 1024000 samples, validate on 256000 samples\n",
      "Epoch 1/5\n",
      " - 724s - loss: 0.5086 - acc: 0.7419 - val_loss: 0.4464 - val_acc: 0.7921\n",
      "Epoch 2/5\n",
      " - 721s - loss: 0.4228 - acc: 0.8064 - val_loss: 0.4312 - val_acc: 0.7997\n",
      "Epoch 3/5\n",
      " - 721s - loss: 0.3908 - acc: 0.8245 - val_loss: 0.4018 - val_acc: 0.8168\n",
      "Epoch 4/5\n",
      " - 720s - loss: 0.3669 - acc: 0.8380 - val_loss: 0.4033 - val_acc: 0.8177\n",
      "Epoch 5/5\n",
      " - 731s - loss: 0.3457 - acc: 0.8493 - val_loss: 0.3970 - val_acc: 0.8230\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "history = model.train(\n",
    "    training_inputs=X_train,\n",
    "    training_labels=Y_train,\n",
    "    validation_data=[X_dev, Y_dev],\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320000/320000 [==============================] - 65s 202us/step\n",
      "score: 0.39605225\n",
      "acc: 0.82321563\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"score: %.8f\" % (score))\n",
    "print(\"acc: %.8f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
