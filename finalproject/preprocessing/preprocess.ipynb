{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import importlib\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/sentiment140_train.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "\n",
    "data = data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text         object\n",
      "sentiment     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rows: 800000\n",
      "Negative rows: 800000\n"
     ]
    }
   ],
   "source": [
    "print('Positive rows: {}'.format(data[ data['sentiment'] == 4]['sentiment'].size))\n",
    "print('Negative rows: {}'.format(data[ data['sentiment'] == 0]['sentiment'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is 1350598\n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for i, row in enumerate(data.iterrows()):\n",
    "    vocabulary |= set(row[1]['text'].split())\n",
    "print('The vocabulary size is', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=40)\n",
    "train_data, dev_data = train_test_split(train_data, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 1024000\n",
      "test_data: 320000\n",
      "dev_data: 256000\n"
     ]
    }
   ],
   "source": [
    "print('train_data: ' + str(len(train_data)))\n",
    "print('test_data: ' + str(len(test_data)))\n",
    "print('dev_data: ' + str(len(dev_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/train_data.csv', index=False)\n",
    "dev_data.to_csv('../data/dev_data.csv', index=False)\n",
    "test_data.to_csv('../data/test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/train_data.csv', encoding='ISO-8859-1', keep_default_na=False)\n",
    "dev_data = pd.read_csv('../data/dev_data.csv', encoding='ISO-8859-1', keep_default_na=False)\n",
    "test_data = pd.read_csv('../data/test_data.csv', encoding='ISO-8859-1', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_head(df, limit=50):\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        print(row[1]['sentiment'], row[1]['text'])\n",
    "        if i == limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Can someone fix Twitter please??  This smell of aquatic mammals is getting a bit overpowering \n",
      "0 Just walked up the stopped 5 story escalator at peachtree center.  Who knew I'd be getting a workout on the way to class? Urban trek.\n",
      "4 @AgentBooth Yes she did, there's proof. You two lovebirds are going to enjoy it \n",
      "0 @selenagomez i'm so sorry the youtube video you accidentally deleted was the dance video one   i was just about to watch it again...\n",
      "0 want to go home now and come back on Thursday... \n",
      "4 Mmmm... Shamrat has arrived... \n",
      "4 going round my aunites in her pool  x\n",
      "4 @stoob0t Cheers buddy, will do!  Was good seeing you last night and having a bit of a bash in the pit \n",
      "0 @thespunkyone sorry to hear that  I will come kick his ass in 20 days if you want me to! Just say the word and I will add it to my lis ...\n",
      "0 I think it's time to retire the Saab. \n",
      "4 is just about to watch one tree hill \n",
      "4 @HolliWinters oh, she's a naughty, naughty girl, darling. I would call fake \n",
      "4 Good morning, Sweet People! \n",
      "4 Last day of school... Teachers of LCS rejoice! \n",
      "0 @arjunghosh oops, sorry dude. Not a rule per se, but these guys do this from time to time \n",
      "0 @sweetliketoffee my mother took my laptop to watch something and my brothers on theirs.... \n",
      "0 @mal_CA i was  but i just got my apt over here in LA babe.. im sad now..so i wont c u for how many years now??\n",
      "4 don't stop believinnnnnnnn! hold on to that feeling \n",
      "0 cannot work this and is thinking of giving up! \n",
      "0 @BobbySoFamous I feel like a jerk now \n",
      "0 @Rai7Rai good morning! Yeah my head still hurts \n",
      "0 there's 173 people missing out on sayinf hi to me ...  well I'll get back to those who did tonight ;) still busy sorting my stuff!\n",
      "4 @thelovingkind It would brighten up any work day, I think! \n",
      "0 is going to be pink for her graduation tomorrow. \n",
      "0 @deltatwo Oh man that sounds so good. There's actually a Publix near me w/much UK candy but it's not near you \n",
      "4 @whykay Congrats \n",
      "0 I cant sleep, but im too sore to move \n",
      "4 Awww good day good day so far. \n",
      "4 Had a fantastic weekend!!!  Lots of sun, the ocean,  bbq'ing....ahhhh.  Another fantastic hot sunny day in Wales today.  \n",
      "4 The 17 again is a wonderful movie. I love it and Zac is very great \n",
      "4 wiiii I hear Radio Disney again \n",
      "4 Is just getting home from work. Long productive day, gonna be longer tommorrow. Love every second of it though. \n",
      "4 @twistednurse76 Morning \n",
      "4 &quot;you and me together, could do anything, babay! you and me together, yes, yes.&quot;    helpin baby study, doc appt, laundry, cuddlin w/ darlin\n",
      "0 @Jpxs15  but you are perpetuating it by including the stupid pussy tag \n",
      "0 i always wake up, craving these juice boxes my mom buys my brother for school. but this time she bought kool aid jammers.  ....\n",
      "0 @Shasuperstar It doesn't look like you and I are on the same Island cause I'm not have BBQ ribs for lunch \n",
      "4 @Candyflip78 I just read that. Thanks man. \n",
      "4 Morning    Im hoping Spurs will win today we need to finish 7th :-/  COYS!!!\n",
      "0 @mileycyrus you saved my life. you taught me life lessons. yet i've still never met you/gotten a reply  i love you miley Ã¢ÂÂ®Ã¢ÂÂ¥\n",
      "4 I want 900 followers by tonight  I'll be your bestest best friend in the world!\n",
      "4 @TkVu Good luck on your exams \n",
      "0 one of the fish has died \n",
      "4 Enjoying some free Joe's BBQ for my birthday! \n",
      "4 Ur the reason i can't stop smiling...   ..&quot;and i like it&quot;\n",
      "4 morning tweeps  hope you are all aven a good start to the week i'm just chilling at home working night shift this week again\n",
      "4 a close 2nd too... lol \n",
      "4 @Branduno: WE GET IT. YOU DON'T LIKE MS AND WOULD PREFER TO PLAY OUTSIDE. HERE'S A COOKIE, NOW SHUT UP \n",
      "4 @wo_ai  if you say it like that, it sounds like i'm the new portuguese PM!  i'll just be the portuguese country manager for our brand \n",
      "0 @RoRossonera where are you hiding? \n",
      "4 @Jonasbrothers thats for loving us \n"
     ]
    }
   ],
   "source": [
    "print_df_head(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove twitter @ tags\n",
    "def remove_twitter_at(text):\n",
    "    # Get rid of the leading @\n",
    "    text = re.sub('^@.*? ', '', text)\n",
    "    \n",
    "    # Get rid of @ in the middle of the tweet\n",
    "    text = re.sub(' @.*? ', ' ', text)\n",
    "    \n",
    "    # Get rid of tailing @\n",
    "    text = re.sub(' @.*?$', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(remove_twitter_at)\n",
    "test_data['text'] = test_data['text'].apply(remove_twitter_at)\n",
    "dev_data['text'] = dev_data['text'].apply(remove_twitter_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to map punctuations to None\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "        if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(remove_punctuations)\n",
    "test_data['text'] = test_data['text'].apply(remove_punctuations)\n",
    "dev_data['text'] = dev_data['text'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only remove separate digits, not digits within word, such as `U2`\n",
    "def remove_digits(text):\n",
    "    # Remove leading digits\n",
    "    text = re.sub('^\\d+? ', ' ', text)\n",
    "    \n",
    "    # Remove other digits\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(remove_digits)\n",
    "test_data['text'] = test_data['text'].apply(remove_digits)\n",
    "dev_data['text'] = dev_data['text'].apply(remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(to_lowercase)\n",
    "test_data['text'] = test_data['text'].apply(to_lowercase)\n",
    "dev_data['text'] = dev_data['text'].apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a string to a list of words\n",
    "def split(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(split)\n",
    "test_data['text'] = test_data['text'].apply(split)\n",
    "dev_data['text'] = dev_data['text'].apply(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/Is-there-a-stop-word-list-specifically-designed-for-sentiment-analysis\n",
    "# As stated before, the general nltk stopwords may have a negative impacts on sentiment analysis, as it removes\n",
    "# negation words such as don't. Here I build a minimal set of stopwords specific to sentiment analysis. \n",
    "\n",
    "stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n",
    "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", \n",
    "    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and'\n",
    "]\n",
    "\n",
    "def remove_stopwords(arr):\n",
    "    arr = [word for word in arr if word not in stopwords]\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
    "test_data['text'] = test_data['text'].apply(remove_stopwords)\n",
    "dev_data['text'] = dev_data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Transform word from the derived form to root form when applicable \n",
    "def stem_words(arr):\n",
    "    arr = [stemmer.stem(word) for word in arr]\n",
    "    return arr\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(stem_words)\n",
    "test_data['text'] = test_data['text'].apply(stem_words)\n",
    "dev_data['text'] = dev_data['text'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_vocabulary(df):\n",
    "    vocabulary = Counter()\n",
    "    for _, row in df.iterrows():\n",
    "        words = row['text']\n",
    "        for word in words:\n",
    "            vocabulary[word] += 1\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = get_corpus_vocabulary(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training corpus: 332848\n",
      "The top 10 words are: [(361347, 'to'), (137945, 'for'), (136893, 'in'), (117479, 'of'), (113318, 'im'), (106876, 'on'), (96571, 'so'), (88166, 'go'), (81575, 'but'), (79925, 'just')]\n",
      "The least 10 words are: [(1, '$$$hip'), (1, '$$$giant'), (1, '$$$down'), (1, '$$$+'), (1, '$$$$$they'), (1, '$$$$$$fgdf^amp^^^^'), (1, '$$$$$$$$$$$$$'), (1, '$$$$$$$$$$$$'), (1, '$$$$$$$$$$$'), (1, '$$$$$$$$$$')]\n",
      "[1, 1, 1, 2, 2, 3, 4, 5, 6, 6, 7, 9, 10, 11, 12, 14, 15, 17, 19, 20, 22, 24, 26, 29, 31, 34, 37, 40, 43, 46, 49, 52, 56, 60, 63, 67, 72, 76, 81, 86, 92, 97, 103, 110, 117, 124, 132, 141, 151, 161, 171, 182, 193, 206, 219, 233, 248, 264, 280, 298, 317, 338, 359, 383, 409, 437, 466, 500, 536, 577, 622, 672, 727, 788, 854, 929, 1013, 1108, 1215, 1337, 1476, 1636, 1821, 2038, 2299, 2609, 2987, 3453, 4043, 4802, 5825, 7239, 9361, 12820, 19097, 32051, 63058, 138609, 235729, 332848]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_vocabulary_summary(vocabulary):\n",
    "    corpus_total_cnt = sum(vocabulary.values())\n",
    "    sorted_by_wd_cnt = sorted(((cnt, wd) for wd, cnt in vocabulary.items()), reverse=True)\n",
    "\n",
    "    plt_points = []\n",
    "    accu = 0\n",
    "    j = 0\n",
    "    for i in range(1, 101):\n",
    "        while accu < (corpus_total_cnt * i / 100.0):\n",
    "            accu += sorted_by_wd_cnt[j][0]\n",
    "            j += 1\n",
    "        plt_points.append(j)\n",
    "\n",
    "    print('The size of the training corpus: {}'.format(len(vocabulary)))\n",
    "    print('The top 10 words are: {}'.format(sorted_by_wd_cnt[:10]))\n",
    "    print('The least 10 words are: {}'.format(sorted_by_wd_cnt[-10:]))\n",
    "    print(plt_points)\n",
    "    plt.bar(range(1, 101), plt_points)\n",
    "    \n",
    "show_vocabulary_summary(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20000 words account for 94% word occurance of the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(arr):\n",
    "    return ' '.join(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(join)\n",
    "test_data['text'] = test_data['text'].apply(join)\n",
    "dev_data['text'] = dev_data['text'].apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 can someone fix twitter please??  this smell of aquatic mammals is getting a bit overpowering \n",
      "0 just walked up the stopped 5 story escalator at peachtree center.  who knew i'd be getting a workout on the way to class? urban trek.\n",
      "4 yes she did, there's proof. you two lovebirds are going to enjoy it \n",
      "0 i'm so sorry the youtube video you accidentally deleted was the dance video one   i was just about to watch it again...\n",
      "0 want to go home now and come back on thursday... \n",
      "4 mmmm... shamrat has arrived... \n",
      "4 going round my aunites in her pool  x\n",
      "4 cheers buddy, will do!  was good seeing you last night and having a bit of a bash in the pit \n",
      "0 sorry to hear that  i will come kick his ass in 20 days if you want me to! just say the word and i will add it to my lis ...\n",
      "0 i think it's time to retire the saab. \n",
      "4 is just about to watch one tree hill \n",
      "4 oh, she's a naughty, naughty girl, darling. i would call fake \n",
      "4 good morning, sweet people! \n",
      "4 last day of school... teachers of lcs rejoice! \n",
      "0 oops, sorry dude. not a rule per se, but these guys do this from time to time \n",
      "0 my mother took my laptop to watch something and my brothers on theirs.... \n",
      "0 i was  but i just got my apt over here in la babe.. im sad now..so i wont c u for how many years now??\n",
      "4 don't stop believinnnnnnnn! hold on to that feeling \n",
      "0 cannot work this and is thinking of giving up! \n",
      "0 i feel like a jerk now \n",
      "0 good morning! yeah my head still hurts \n",
      "0 there's 173 people missing out on sayinf hi to me ...  well i'll get back to those who did tonight ;) still busy sorting my stuff!\n",
      "4 it would brighten up any work day, i think! \n",
      "0 is going to be pink for her graduation tomorrow. \n",
      "0 oh man that sounds so good. there's actually a publix near me w/much uk candy but it's not near you \n",
      "4 congrats \n",
      "0 i cant sleep, but im too sore to move \n",
      "4 awww good day good day so far. \n",
      "4 had a fantastic weekend!!!  lots of sun, the ocean,  bbq'ing....ahhhh.  another fantastic hot sunny day in wales today.  \n",
      "4 the 17 again is a wonderful movie. i love it and zac is very great \n",
      "4 wiiii i hear radio disney again \n",
      "4 is just getting home from work. long productive day, gonna be longer tommorrow. love every second of it though. \n",
      "4 morning \n",
      "4 &quot;you and me together, could do anything, babay! you and me together, yes, yes.&quot;    helpin baby study, doc appt, laundry, cuddlin w/ darlin\n",
      "0  but you are perpetuating it by including the stupid pussy tag \n",
      "0 i always wake up, craving these juice boxes my mom buys my brother for school. but this time she bought kool aid jammers.  ....\n",
      "0 it doesn't look like you and i are on the same island cause i'm not have bbq ribs for lunch \n",
      "4 i just read that. thanks man. \n",
      "4 morning    im hoping spurs will win today we need to finish 7th :-/  coys!!!\n",
      "0 you saved my life. you taught me life lessons. yet i've still never met you/gotten a reply  i love you miley ã¢ââ®ã¢ââ¥\n",
      "4 i want 900 followers by tonight  i'll be your bestest best friend in the world!\n",
      "4 good luck on your exams \n",
      "0 one of the fish has died \n",
      "4 enjoying some free joe's bbq for my birthday! \n",
      "4 ur the reason i can't stop smiling...   ..&quot;and i like it&quot;\n",
      "4 morning tweeps  hope you are all aven a good start to the week i'm just chilling at home working night shift this week again\n",
      "4 a close 2nd too... lol \n",
      "4 we get it. you don't like ms and would prefer to play outside. here's a cookie, now shut up \n",
      "4  if you say it like that, it sounds like i'm the new portuguese pm!  i'll just be the portuguese country manager for our brand \n",
      "0 where are you hiding? \n",
      "4 thats for loving us \n"
     ]
    }
   ],
   "source": [
    "print_df_head(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      "[0 4]\n",
      "test_data:\n",
      "[4 0]\n",
      "dev_data:\n",
      "[0 4]\n"
     ]
    }
   ],
   "source": [
    "print('train_data:')\n",
    "print(train_data['sentiment'].unique())\n",
    "print('test_data:')\n",
    "print(test_data['sentiment'].unique())\n",
    "print('dev_data:')\n",
    "print(dev_data['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_by_empty_str(text):\n",
    "    if text == np.nan:\n",
    "        return ''\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['text'].apply(replace_nan_by_empty_str)\n",
    "test_data['text'] = test_data['text'].apply(replace_nan_by_empty_str)\n",
    "dev_data['text'] = dev_data['text'].apply(replace_nan_by_empty_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 1024000\n",
      "dev_data: 256000\n",
      "test_data: 320000\n"
     ]
    }
   ],
   "source": [
    "print('train_data: ' + str(len(train_data)))\n",
    "print('dev_data: ' + str(len(dev_data)))\n",
    "print('test_data: ' + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/fully_cleansed_train_data.csv', index=False)\n",
    "dev_data.to_csv('../data/fully_cleansed_dev_data.csv', index=False)\n",
    "test_data.to_csv('../data/fully_cleansed_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/no_stemming_train_data.csv', index=False)\n",
    "dev_data.to_csv('../data/no_stemming_dev_data.csv', index=False)\n",
    "test_data.to_csv('../data/no_stemming_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/no_tags_train_data.csv', index=False)\n",
    "dev_data.to_csv('../data/no_tags_dev_data.csv', index=False)\n",
    "test_data.to_csv('../data/no_tags_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/no_tags_lower_train_data.csv', index=False)\n",
    "dev_data.to_csv('../data/no_tags_lower_dev_data.csv', index=False)\n",
    "test_data.to_csv('../data/no_tags_lower_test_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
